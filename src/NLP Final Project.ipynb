{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 6120 - Natural Language Processing\n",
    "#### Author - Shubhanshu Gupta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install emoji --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import emoji\n",
    "from wordcloud import WordCloud\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from lightgbm import LGBMClassifier\n",
    "from nltk import word_tokenize\n",
    "from sklearn import metrics,preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle as pk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,SpatialDropout1D,Bidirectional,GRU,GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import  Adam\n",
    "import keras\n",
    "import gensim\n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.callbacks import Callback,ReduceLROnPlateau\n",
    "from sklearn import metrics,preprocessing\n",
    "from keras.models import load_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finding the years in data\n",
    "def convert_year(year):\n",
    "  yr = year.split('-')[0]\n",
    "  return int(yr)\n",
    "train_df['year'] = train_df['created_date'].apply(lambda x: int(x.split('-')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting target to boolean form\n",
    "train_df['bool_target'] = train_df['target'].apply(lambda x: 1 if x >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping columns\n",
    "train_df.drop(['id','created_date','publication_id','parent_id','article_id','identity_annotator_count','toxicity_annotator_count'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessing:\n",
    "    \n",
    "    EMOJI_REGEXP = emoji.get_emoji_regexp()\n",
    "\n",
    "    UNICODE_EMOJI_MY = {\n",
    "        k: f\" EMJ {v.strip(':').replace('_', ' ')} \"\n",
    "        for k, v in emoji.UNICODE_EMOJI_ALIAS.items()\n",
    "    }\n",
    "    \n",
    "    REGEX_REPLACER = [\n",
    "        (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n",
    "        for pat, repl in WORDS_REPLACER\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    ## Removing html elemnets from the descriptions and features\n",
    "    def cleanhtml(self,raw_html):\n",
    "        cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "        cleantext = re.sub(cleanr, '', raw_html)\n",
    "        return cleantext\n",
    "    \n",
    "    ## Clean numbers from the text\n",
    "    def clean_numbers(x):\n",
    "        return re.sub(r'\\d+', ' ', x)\n",
    "    \n",
    "    # Removing punctuations from the text\n",
    "    def remove_punctuations(self,text):\n",
    "        text = re.sub(r'[^a-zA-z\\s]', '', text)\n",
    "        return text\n",
    "    \n",
    "    # Removing all special_characters except english alphabets with option to remove digits\n",
    "    def remove_special_characters(self,text, remove_digits=False):\n",
    "        pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        return text\n",
    "    \n",
    "    # replacing given text to given pattern \n",
    "    def replace_regex(self,text,pattern,replacement):\n",
    "        return re.sub(pattern,replacement,text)\n",
    "    \n",
    "    ## Replacing Emoji with text\n",
    "    def my_demojize(self,string):\n",
    "        def replace(match):\n",
    "            return UNICODE_EMOJI_MY.get(match.group(0), match.group(0))\n",
    "\n",
    "        return re.sub(\"\\ufe0f\", \"\", EMOJI_REGEXP.sub(replace, string))\n",
    "    \n",
    "    ## Replacing starred words in the text\n",
    "    def replace_starred(self,text):\n",
    "        for pattern, repl in REGEX_REPLACER:\n",
    "            text = pattern.sub(repl, text)\n",
    "        return text\n",
    "      \n",
    "    # expandcontractions such as isn't to is not\n",
    "    def expand_contractions(self,text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "            expanded_contraction = first_char+expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "        \n",
    "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "        return expanded_text\n",
    "        \n",
    "    # Splitting joined english words like 'themoney' to 'the money'\n",
    "    def split_words(self,text):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        filtered_tokens = [' '.join(wordninja.split(token)) for token in tokens]        \n",
    "        filtered_text = ' '.join(filtered_tokens)    \n",
    "        return filtered_text\n",
    "    \n",
    "    ## Function used for text preprocessing\n",
    "    def text_preprocessing(self,text):\n",
    "        text = self.cleanhtml(text)\n",
    "        text = text.lower()\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.my_demojize(text)\n",
    "        text = replace_regex(text,\"\\\\S+@\\\\S+\",\"\")\n",
    "        text = replace_regex(text,\"[Hh]ttp([^ ]+)\",\"\")\n",
    "        text = replace_regex(text,\"RT | via\",\"\")\n",
    "        text = replace_regex(text,\"@([^ ]+)\",\"\")\n",
    "        text = replace_regex(text,\"[Ww]ww([^ ]+)\",\"\")\n",
    "        text = replace_regex(text,\"[@][a - zA - Z0 - 9_]{1,15}\",\"\")\n",
    "        text = self.replace_starred(text)\n",
    "        text = self.remove_special_characters(text)\n",
    "        text = self.clean_numbers(text)\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        # stripping extra space\n",
    "        text = text.strip()\n",
    "        text = text.lower()\n",
    "\n",
    "        return text\n",
    "    \n",
    "text_processing = TextProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['comment_text'] = train_df['comment_text'].apply(lambda text: text_processing.text_preprocessing(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting percentage of toxic and non-toxic comments\n",
    "\n",
    "ax = sns.countplot(train_df['bool_target'], palette='Set3')\n",
    "for p in ax.patches:\n",
    "  height = p.get_height()\n",
    "  ax.text(p.get_x()+p.get_width()/2.,height + 3,'{:1.2f}%'.format(100*height/float(len(train_df))),ha=\"center\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwrds = set(stopwords.words(\"english\"))\n",
    "\n",
    "### Function to plot word clouds\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        stopwords=stopwrds,\n",
    "        max_words=70,\n",
    "        max_font_size=30, \n",
    "        scale=5,\n",
    "        random_state=1\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(10,10))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wordcloud on full text\n",
    "show_wordcloud(train_df['comment_text'], title = 'Most common Words in training corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wordcloud on toxic comments\n",
    "show_wordcloud(train_df[train_df['bool_target'] == 1]['comment_text'], title = 'Most common Words in training corpus with toxic comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wordcloud on  non-toxic comments\n",
    "show_wordcloud(train_df[train_df['bool_target'] == 0]['comment_text'], title = 'Most common Words in training corpus with toxic comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Number of comments in eachyear\n",
    "sns.countplot(x=\"year\", hue=\"bool_target\", data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting percentage of toxic comments related to different identities\n",
    "demographics = orig_train.loc[:, ['target']+list(orig_train)[slice(7,31)]].dropna()\n",
    "weighted_toxic = demographics.iloc[:, 1:].multiply(demographics.iloc[:, 0], axis=\"index\").sum()/demographics.iloc[:, 1:][demographics.iloc[:, 1:]>0].count()\n",
    "weighted_toxic = weighted_toxic.sort_values(ascending=False)\n",
    "plt.figure(figsize=(30,20))\n",
    "sns.set(font_scale=3)\n",
    "ax = sns.barplot(x = weighted_toxic.values, y = weighted_toxic.index, alpha=0.8)\n",
    "plt.ylabel('Demographics')\n",
    "plt.xlabel('Weighted Toxic')\n",
    "plt.title('Percent of toxic comments related to different identities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions to create evaluation metric\n",
    "def evaluation_metric_subgroups(df,groups):\n",
    "    if groups == 'All': \n",
    "        #These are the whole identities \n",
    "        categoriese = ['physical_disability','psychiatric_or_mental_illness','jewish','asian','homosexual_gay_or_lesbian','black','muslim','white','christian','female','male']\n",
    "    else:\n",
    "        categoriese = groups\n",
    "    categoriese_df = pd.DataFrame(columns = ['SUB','BPSN','BNSP'], index = categoriese)\n",
    "\n",
    "    for category in categoriese:\n",
    "        #change it to 0 or 1 rather than probabilities\n",
    "        #if the identity is mentioned or not\n",
    "        #if the category is NA, treated it as 0\n",
    "        df[category] = df[category] >= 0.5\n",
    "        #calculate the subgroup AUC\n",
    "        #it is possible that there is no data, then we will just assign each value to be 0\n",
    "        if df[df[category]].shape[0] == 0:\n",
    "            #remove the entire row\n",
    "             categoriese_df = categoriese_df.drop(category, axis = 0)\n",
    "        else:\n",
    "            categoriese_df.loc[category,'SUB'] = auc(df[df[category]])\n",
    "            bpsn = ((~df[category] & df['bool_target'])    #background positive\n",
    "                | (df[category] & ~df['bool_target'])) #subgroup negative\n",
    "            categoriese_df.loc[category,'BPSN'] = auc(df[bpsn])\n",
    "            bnsp = ((~df[category] & ~df['bool_target'])   #background negative\n",
    "                | (df[category] & df['bool_target']))  #subgrooup positive\n",
    "            categoriese_df.loc[category,'BNSP'] = auc(df[bnsp])\n",
    "    #drop rows that contain NANs due to insufficient data\n",
    "    categoriese_df = categoriese_df.dropna(axis = 0)\n",
    "\n",
    "    #Apply the power function defined before\n",
    "    categoriese_df.loc['Mp',:] = categoriese_df.apply(Mp, axis= 0)\n",
    "\n",
    "    return categoriese_df\n",
    "\n",
    "def Mp(data, p = -5.0):\n",
    "    return np.average(data ** p) ** (1/p)\n",
    "\n",
    "def auc(df):\n",
    "    y_orig = df['bool_target']\n",
    "    y_pred = df['prediction'] \n",
    "    fpr, tpr, thresholds = roc_curve(y_orig,y_pred)\n",
    "    return metrics.auc(fpr, tpr)\n",
    "\n",
    "def final_metric(df,groups= 'All'):\n",
    "    #First of all, swtich the probability to labels\n",
    "    y_orig = df['bool_target']\n",
    "    y_pred = df['prediction']  \n",
    "    w0,w1,w2,w3 = 0.25,0.25,0.25,0.25\n",
    "\n",
    "    #Next caluclate the overall arc and the Mp of each sub-metrics\n",
    "    overall = auc(df)\n",
    "    categoriese_df = evaluation_metric_subgroups(df,groups)\n",
    "    final_metric = w0 * overall + w1 * categoriese_df.loc['Mp','SUB'] + w2 * categoriese_df.loc['Mp','BPSN'] + w3 * categoriese_df.loc['Mp','BNSP']\n",
    "\n",
    "    return final_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading sampled data\n",
    "train_df = pd.read_csv('model_train.csv')\n",
    "test_df = pd.read_csv('model_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
    "word_vectorizer.fit(complete_text)\n",
    "train_text_split = train_df['comment_text'].apply(lambda x: np.str(x))\n",
    "test_text_split = test_df['comment_text'].apply(lambda x: np.str(x))\n",
    "y_orig = train_df['bool_target']\n",
    "train_features = word_vectorizer.transform(train_text_split)\n",
    "test_features = word_vectorizer.transform(test_text_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Smote and RandomSampler for creating data balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(Counter(y_orig))\n",
    "train_features = word_vectorizer.transform(train_text_split)\n",
    "\n",
    "sampler = make_pipeline(\n",
    "    RandomUnderSampler(random_state=0, sampling_strategy=0.2),\n",
    "    SMOTE(random_state=0)\n",
    ")\n",
    "train_features,y_orig = sampler.fit_resample(train_features,y_orig)   \n",
    "\n",
    "print(Counter(y_orig))\n",
    "\n",
    "test_features = word_vectorizer.transform(test_text_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(solver='saga', random_state = 10,n_jobs= 8)\n",
    "classifier.fit(train_features,y_orig)\n",
    "pred_target = classifier.predict_proba(test_features)[:, 1]\n",
    "prediction_log = {'id': test_df.index}\n",
    "prediction_log['prediction'] = classifier.predict_proba(test_features)[:, 1]\n",
    "prediction_log = pd.DataFrame.from_dict(prediction_log)\n",
    "prediction_log['prediction'] = prediction_log['prediction'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "merged_log = test_df.merge(prediction_log, left_index=True, right_index=True).fillna(0)\n",
    "\n",
    "# Printing the results\n",
    "print(f'Final Metric: {final_metric(merged_log)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_NB = MultinomialNB()\n",
    "classifier_NB.fit(train_features,y_orig)\n",
    "prediction_nb = {'id': test_df.index}\n",
    "prediction_nb['prediction'] = classifier_NB.predict_proba(test_features)[:, 1]\n",
    "prediction_nb = pd.DataFrame.from_dict(prediction_nb)\n",
    "prediction_nb['prediction'] = prediction_nb['prediction'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "merged_nb = test_df.merge(prediction_nb, left_index=True, right_index=True).fillna(0)\n",
    "\n",
    "# Printing the results\n",
    "print(f'Final Metric: {final_metric(merged_nb)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting with Tf- IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_LGBM = LGBMClassifier(n_jobs = 8)\n",
    "classifier_LGBM.fit(train_features,y_orig)\n",
    "prediction_lgbm = {'id': test_df.index}\n",
    "prediction_lgbm['prediction'] = classifier_LGBM.predict_proba(test_features)[:, 1]\n",
    "prediction_lgbm = pd.DataFrame.from_dict(prediction_lgbm)\n",
    "prediction_lgbm['prediction'] = prediction_lgbm['prediction'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "merged_lgbm = test_df.merge(prediction_lgbm, left_index=True, right_index=True).fillna(0)\n",
    "\n",
    "# Printing the results\n",
    "print(f'Final Metric: {final_metric(merged_lgbm)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding and creating data for Glove Embedding layer and for LSTM models \n",
    "\n",
    "X_train = train_df['comment_text'].values\n",
    "y_train = train_df['bool_target'].values\n",
    "X_test = test_df['comment_text'].values\n",
    "y_test = test_df['bool_target'].values\n",
    "\n",
    "total_comment = np.concatenate([X_train, X_test])\n",
    "\n",
    "max_length = max([len(com.split()) for com in total_comment])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(total_comment)\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length)\n",
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating functions for metrics\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    keras.metrics.Precision(name=\"precision\"),\n",
    "    keras.metrics.Recall(name=\"recall\"),\n",
    "    keras.metrics.AUC(name=\"auc\"),\n",
    "    f1_m\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating callback to save best model based on our metric\n",
    "global_score = 0\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "#pd.options.mode.chained_assignment = None\n",
    "\n",
    "class RocAucEvaluation(keras.callbacks.Callback):\n",
    "  \n",
    "  def __init__(self, model, validation_data=()):\n",
    "\n",
    "    #kwargs['monitor'] = 'val_bias_metric'\n",
    "        \n",
    "    self.model = model\n",
    "    self.X_val, self.y_val = validation_data\n",
    "        #super().__init__(*args, **kwargs)\n",
    "            \n",
    "  def on_epoch_end(self, epoch,logs = {}):\n",
    "    global global_score\n",
    "    prediction = {'id': test_df.index}\n",
    "    prediction['prediction'] = self.model.predict(self.X_val,batch_size=256)[:,0]\n",
    "    prediction = pd.DataFrame.from_dict(prediction)\n",
    "    prediction['prediction'] = prediction['prediction'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "    merged = test_df.merge(prediction, left_index=True, right_index=True).fillna(0)\n",
    "    score = final_metric(merged)\n",
    "    \n",
    "    if score > global_score:\n",
    "      model_json = model.to_json()\n",
    "      with open(\"final.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "      model.save(\"final.h5\")\n",
    "      print(\"\\n Saved model to disk\")\n",
    "      global_score = score\n",
    "\n",
    "    logs['val_bias_metric'] = score\n",
    "    print(\"\\n ROC-AUC - score: {:.6f}\".format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://usmlproject.s3.amazonaws.com/glove.42B.300d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating embedding details\n",
    "\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating weights for each class\n",
    "counts = np.bincount(y_train)\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]\n",
    "\n",
    "#print(weight_for_0,weight_for_1)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Two LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size,EMBEDDING_DIM,input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
    "model.add(LSTM(128,dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.Adam(1e-2),metrics=metrics)\n",
    "\n",
    "es = RocAucEvaluation(model, validation_data=(X_test_pad, y_test))\n",
    "rlrp = ReduceLROnPlateau(monitor='val_auc', factor=0.2, patience=3, min_delta=1E-7)\n",
    "\n",
    "model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    batch_size=256,\n",
    "    epochs=30,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[es,rlrp]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Two LSTM layer with CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size,EMBEDDING_DIM,input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128,return_sequences=True,dropout=0.2))\n",
    "model.add(LSTM(128,dropout=0.2))\n",
    "model.add(Conv1D(64, kernel_size = 5, padding = \"valid\", kernel_initializer = \"he_uniform\"))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.Adam(1e-2),metrics=metrics)\n",
    "\n",
    "model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    batch_size=256,\n",
    "    epochs=30,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[es,rlrp]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size,EMBEDDING_DIM,input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(128,dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64,dropout=0.2, return_sequences=True)))\n",
    "# model.add(Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\"))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.Adam(1e-2),metrics=metrics)\n",
    "\n",
    "model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    batch_size=256,\n",
    "    epochs=30,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[es,rlrp]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size,EMBEDDING_DIM,input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(128,dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64,dropout=0.2, return_sequences=True)))\n",
    "model.add(Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\"))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.Adam(1e-2),metrics=metrics)\n",
    "\n",
    "model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    batch_size=256,\n",
    "    epochs=30,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[es,rlrp]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidirectional GRU with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size,EMBEDDING_DIM,input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(GRU(128,dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(64,dropout=0.2, return_sequences=True)))\n",
    "model.add(Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\"))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.Adam(1e-2),metrics=metrics)\n",
    "\n",
    "model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    batch_size=256,\n",
    "    epochs=30,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    class_weight=class_weight,\n",
    "    callbacks=[es,rlrp]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on final set (this is repeated for all above methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('test.csv')\n",
    "X_test_final = test_set['comment_text'].values\n",
    "X_test_tokens_final = tokenizer.texts_to_sequences(X_test_final)\n",
    "X_test_pad_final = pad_sequences(X_test_tokens_final, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = {'id': test_df.index}\n",
    "prediction['prediction'] = model.predict(X_test_pad_final,batch_size=256)[:,0]\n",
    "prediction = pd.DataFrame.from_dict(prediction)\n",
    "prediction['prediction'] = prediction['prediction'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "merged = test_df.merge(prediction, left_index=True, right_index=True).fillna(0)\n",
    "# Printing the results\n",
    "print(f'Final Metric: {final_metric(merged)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other methods that were tried were LSTM without embedding,SVM but it took too long to run even after dimensionality reduction using SVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
