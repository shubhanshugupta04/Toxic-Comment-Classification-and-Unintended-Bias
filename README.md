# Toxic-Comment-Classification-and-Unintended-Bias


### Abstract
In this project we worked on Jigsaw Unintended Bias in Toxicity Classification Challenge on Kaggle. We built an unbiased classification model to identify toxicity in online comments using machine learning models such as Logistic Regression and Naive Bayes, and deep learning models mostly making use of RNN and LSTM. Our best-performing model achieves 0.809 generalized AUC on the test set.

### Summary
There has been an exponential rise in the number of people who use social media every day. With more and more people gaining access to social media sites, there is an increase in the number of posts, comments and interactions posted every day. With increase in number of posts, there is also a huge increase in the amount of toxicity that is being put every day on these sites. Toxicity can be defined as an insult, threat, identity attack or just pure obscenity. These kinds of comments are known to be more targeted towards certain groups of people or identities. Social media sometimes fail to moderate such content especially in cases where there is bias against some community for example, moderation models might associate a non-toxic comment like “as a gay man” and a toxic comment like “why are you acting gay?” together which creates a bias against people from the LGBTQIA community. Our objective is to develop moderation models that can find toxic comments or posts on social media sites and identify bias against people from certain communities. Also, we aim to find out the communities which face the highest amount of bias.

